# =============================================================================
# LangGraph Chatbot Agent - Configuration
# =============================================================================
# Production-ready configuration for the AI chatbot agent.
# Override values using environment variables with the same name (uppercase).
#
# Example: llm.model -> LLM_MODEL environment variable
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Configuration
# -----------------------------------------------------------------------------
llm:
  # Provider: "openai", "anthropic", "bedrock", "ollama"
  provider: "bedrock"

  # Model identifier
  model: "us.anthropic.claude-3-5-sonnet-20241022-v2:0"

  # Temperature for response generation (0.0-1.0)
  # Lower = more deterministic, Higher = more creative
  temperature: 0.7

  # Maximum tokens for response generation
  max_tokens: 4096

  # Top-p sampling (nucleus sampling)
  top_p: 0.9

  # Request timeout in seconds
  timeout_seconds: 120

  # Streaming configuration
  streaming:
    enabled: true
    chunk_size: 10  # tokens per chunk
    heartbeat_interval: 5  # seconds between heartbeats

# -----------------------------------------------------------------------------
# Agent Configuration
# -----------------------------------------------------------------------------
agent:
  # Maximum iterations for agent loops (prevents infinite loops)
  max_iterations: 10

  # Overall timeout for agent execution
  timeout_seconds: 300

  # Enable streaming responses
  enable_streaming: true

  # Human-in-the-loop configuration
  human_in_loop:
    enabled: true
    # Nodes that require human approval before execution
    interrupt_before:
      - "calendar_confirmation"
      - "delete_confirmation"
    # Timeout for human approval (seconds)
    approval_timeout: 3600

  # Self-correction configuration
  self_correction:
    enabled: true
    max_retries: 2
    confidence_threshold: 0.5
    hallucination_threshold: 0.3

  # Rate limiting for LLM calls
  rate_limits:
    max_llm_calls: 10
    max_cost_usd: 1.0
    tokens_per_minute: 100000

# -----------------------------------------------------------------------------
# Graph Architecture
# -----------------------------------------------------------------------------
graph:
  # Entry point node
  entry_point: "entry_node"

  # Available graph types
  default_graph: "docscheduler"  # "rag" or "docscheduler"

  # Node execution timeouts (seconds)
  node_timeouts:
    entry_node: 5
    router_node: 10
    query_analysis: 10
    query_enhancement: 15
    retrieval: 30
    reranking: 20
    generation: 60
    human_review: 3600
    response_formatting: 10
    logging: 5

  # Conditional edge configurations
  routing:
    # Minimum confidence for routing decisions
    confidence_threshold: 0.7

    # Default route if classification fails
    fallback_route: "general_chat"

    # Route classifications
    classifications:
      - document
      - calendar
      - general
      - greeting
      - human_approval

# -----------------------------------------------------------------------------
# Tools Configuration
# -----------------------------------------------------------------------------
tools:
  # Document search tools
  document_search:
    # Number of documents to retrieve initially
    top_k: 20

    # Number of documents after reranking
    rerank_top_k: 5

    # Minimum similarity score (0.0-1.0)
    similarity_threshold: 0.05

    # Search strategies: similarity, mmr, hybrid, ensemble
    strategy: "hybrid"

    # Hybrid search weights
    vector_weight: 0.4
    keyword_weight: 0.6

  # Calendar tools
  calendar:
    # Default meeting duration
    default_duration_minutes: 60

    # Timezone for scheduling
    timezone: "UTC"

    # Working hours for availability checks
    working_hours:
      start: 9
      end: 17

    # Buffer time between meetings (minutes)
    buffer_minutes: 15

    # Maximum attendees per meeting
    max_attendees: 10

    # Auto-add Google Meet links
    auto_add_meet: true

    # Send email notifications
    send_notifications: true

  # Summarization tools
  summarization:
    max_length: 500
    min_length: 100
    style: "concise"  # concise, detailed, bullet_points

# -----------------------------------------------------------------------------
# Persistence Configuration
# -----------------------------------------------------------------------------
persistence:
  # Checkpointer type: "postgresql", "redis", "memory"
  checkpointer: "postgresql"

  # Checkpoint retention (days)
  checkpoint_ttl_days: 30

  # Connection pool settings
  pool:
    min_size: 2
    max_size: 10
    max_idle_time: 300

  # Thread management
  threads:
    max_per_user: 100
    auto_cleanup: true
    cleanup_after_days: 90

# -----------------------------------------------------------------------------
# Memory Configuration
# -----------------------------------------------------------------------------
memory:
  # Short-term (working) memory
  short_term:
    max_messages: 20
    max_tokens: 4000

  # Long-term memory
  long_term:
    enabled: true
    # Store in database
    persistence: "postgresql"
    # Entity extraction
    extract_entities: true
    entity_ttl_days: 30

  # Conversation summarization
  summarization:
    enabled: true
    threshold_messages: 20
    max_summary_tokens: 500

# -----------------------------------------------------------------------------
# Retrieval Configuration
# -----------------------------------------------------------------------------
retrieval:
  # Vector store settings
  vector_store:
    type: "pgvector"
    embedding_dimension: 768
    index_type: "ivfflat"  # ivfflat, hnsw

  # Embedding model
  embeddings:
    provider: "huggingface"
    model: "sentence-transformers/all-mpnet-base-v2"
    batch_size: 100

  # Reranking configuration
  reranking:
    enabled: true
    method: "cross_encoder"  # cross_encoder, rrf, llm
    model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

  # Query enhancement
  query_enhancement:
    enabled: true
    expand_synonyms: true
    max_variations: 3

# -----------------------------------------------------------------------------
# Error Handling & Resilience
# -----------------------------------------------------------------------------
error_handling:
  # Retry configuration
  retry:
    max_attempts: 3
    initial_delay_seconds: 2
    max_delay_seconds: 30
    exponential_base: 2

  # Circuit breaker for external APIs
  circuit_breaker:
    enabled: true
    failure_threshold: 5
    recovery_timeout_seconds: 60
    half_open_max_calls: 3

  # Fallback responses
  fallback:
    enabled: true
    message: "I apologize, but I'm having trouble processing your request. Please try again in a moment."

  # Timeout handling
  timeouts:
    default_seconds: 30
    llm_seconds: 120
    tool_seconds: 60

# -----------------------------------------------------------------------------
# Observability Configuration
# -----------------------------------------------------------------------------
observability:
  # LangSmith integration
  langsmith:
    enabled: true
    # Project name in LangSmith
    project: "langraph-chatbot-production"
    # Tracing level: all, errors, samples
    tracing_level: "all"
    # Sample rate for "samples" level (0.0-1.0)
    sample_rate: 0.1

  # Structured logging
  logging:
    level: "INFO"
    format: "json"
    include_timestamps: true
    include_trace_ids: true

  # Metrics collection
  metrics:
    enabled: true
    # Metrics to collect
    collect:
      - latency
      - token_usage
      - error_rates
      - tool_success_rates
      - cache_hit_rates
    # Export interval (seconds)
    export_interval: 60

  # Performance monitoring
  performance:
    enabled: true
    # Track execution times per node
    track_node_times: true
    # Alert thresholds
    alerts:
      latency_p95_ms: 10000
      error_rate_percent: 5
      cost_per_request_usd: 0.50

# -----------------------------------------------------------------------------
# API Configuration
# -----------------------------------------------------------------------------
api:
  # Server settings
  server:
    host: "0.0.0.0"
    port: 8000
    workers: 4

  # CORS settings
  cors:
    allowed_origins:
      - "http://localhost:5173"
      - "http://localhost:3000"
    allow_credentials: true

  # Rate limiting
  rate_limiting:
    enabled: true
    requests_per_minute: 60
    burst_size: 10

  # Request validation
  validation:
    max_message_length: 10000
    max_document_ids: 10
    sanitize_inputs: true

# -----------------------------------------------------------------------------
# Streaming Configuration
# -----------------------------------------------------------------------------
streaming:
  # SSE (Server-Sent Events)
  sse:
    enabled: true
    heartbeat_interval: 5
    timeout_seconds: 3600

  # WebSocket
  websocket:
    enabled: true
    ping_interval: 30
    ping_timeout: 10
    max_message_size: 1048576  # 1MB

  # Event types to stream
  events:
    - status
    - token
    - sources
    - progress
    - complete
    - error
    - node
    - tool
    - meeting

# -----------------------------------------------------------------------------
# Security Configuration
# -----------------------------------------------------------------------------
security:
  # Input validation
  input_validation:
    enabled: true
    max_length: 10000
    sanitize_html: true
    detect_prompt_injection: true

  # Output sanitization
  output_sanitization:
    enabled: true
    remove_pii: false

  # Authentication
  authentication:
    jwt_algorithm: "HS256"
    token_expiry_minutes: 30
    refresh_token_expiry_days: 7

# -----------------------------------------------------------------------------
# Deployment Configuration
# -----------------------------------------------------------------------------
deployment:
  # Environment
  environment: "production"

  # Docker settings
  docker:
    image: "langraph-chatbot:latest"
    port: 8000

  # Health checks
  health:
    startup_probe:
      path: "/health/startup"
      initial_delay: 10
      period: 5
    liveness_probe:
      path: "/health/live"
      period: 10
    readiness_probe:
      path: "/health/ready"
      period: 5

  # Resource limits
  resources:
    memory_limit: "2Gi"
    cpu_limit: "2"
